{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:42.003828Z",
     "start_time": "2018-05-24T08:20:41.999197Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import sys\n",
    "import tempfile\n",
    "from dateutil import tz\n",
    "\n",
    "from ratschlab_common.io.bigmatrix import TablesBigMatrixReader, TablesBigMatrixWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Matrix Usage Example\n",
    "\n",
    "`Bigmatrix` represents an abstraction for out-of-core computation with large\n",
    "matrices having some 'metadata' associated with the rows and columns. This\n",
    "metadata could be for example some patient id and a timestamp for the rows\n",
    "and a column name for each column, e.g. the name of features. The matrix itself\n",
    "could represent the values of the features for patients at different times.\n",
    "\n",
    "The motivation behind this abstraction is, that sometimes a dataset can be almost\n",
    "represented as a matrix, i.e. has the same datatype across all columns, typically\n",
    "some float or int. But a few columns represent something else, e.g. time information\n",
    "or ids. The idea is to handle these metadata separately from the rest. This way\n",
    "the entire dataset can be handled more efficiently and easily.\n",
    "\n",
    "`ratschlab_common.io.bigmatrix` serializes all the data to disk using HDF5 via the `pytables` library. A file generated by this module consists of three nodes: one for the matrix, one for the column description and one for the row description. The matrix is written directly by `pytables` whereas the metadata is written by `pandas`. This can be changed to `pytables` as well, for example in case `pandas` would use too much memory for processing the metadata.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T09:48:52.733583Z",
     "start_time": "2018-05-15T09:48:52.727914Z"
    }
   },
   "source": [
    "## Generating Fake Data\n",
    "\n",
    "First, we'll generate some fake data and write it incrementally using `TablesBigMatrixWriter`.\n",
    "\n",
    "The fake data consists of numerical records of several patients, where each record has a timestamp and a patient id associated. These two columns are stored as row descriptions (i.e. a dataframe). \n",
    "\n",
    "Note, that in principle, the time information could be integrated into the numerical matrix. However, indexing and time zone handling etc would have to be done manually. In this approach, we let `pandas` take care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:42.102626Z",
     "start_time": "2018-05-24T08:20:42.097690Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42**2 # make it deterministic (at least when running from top to bottom)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "## config:\n",
    "num_patients = 50\n",
    "patient_ids = np.random.randint(100000, size=num_patients)\n",
    "\n",
    "rows_per_patient = 1000\n",
    "cols = 150\n",
    "\n",
    "start_date = datetime.datetime(2018, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:42.114461Z",
     "start_time": "2018-05-24T08:20:42.106474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05587935447692871"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size_in_memory = 8*num_patients*rows_per_patient*cols/1024**3 #GB\n",
    "data_size_in_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:42.123386Z",
     "start_time": "2018-05-24T08:20:42.117688Z"
    }
   },
   "outputs": [],
   "source": [
    "def fake_patient_data(pid, rows, cols):\n",
    "    \"\"\"\n",
    "    Generates fake data for one patient with a given patient id\n",
    "    \"\"\"\n",
    "    ids = np.full((rows,), pid)\n",
    "    \n",
    "    # a measurement all 5 minutes\n",
    "    timestamps = pd.date_range(pd.to_datetime(start_date), periods=rows, freq='5min')\n",
    "    \n",
    "    row_desc = pd.DataFrame.from_dict({'id' : ids, 'ts' : timestamps})\n",
    "    \n",
    "    data = np.random.uniform(size=(rows, cols)) + pid # biasing the values, s.t. we know we got the data from the right patient\n",
    "    \n",
    "    return data, row_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:42.134557Z",
     "start_time": "2018-05-24T08:20:42.127244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/var/folders/ry/d9fpyr5d21xgbjyrl084ssqh0000gn/T/tmptkcxmkl4/fake_patient_data.h5')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "output_path = Path(temp_dir.name, 'fake_patient_data.h5')\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also columns can be described using a data frame. Typically, it would contain just the column name. Here we also add some comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:42.186113Z",
     "start_time": "2018-05-24T08:20:42.171112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You could add some comment for column 0</td>\n",
       "      <td>my_col0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You could add some comment for column 1</td>\n",
       "      <td>my_col1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You could add some comment for column 2</td>\n",
       "      <td>my_col2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You could add some comment for column 3</td>\n",
       "      <td>my_col3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You could add some comment for column 4</td>\n",
       "      <td>my_col4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   comment     name\n",
       "0  You could add some comment for column 0  my_col0\n",
       "1  You could add some comment for column 1  my_col1\n",
       "2  You could add some comment for column 2  my_col2\n",
       "3  You could add some comment for column 3  my_col3\n",
       "4  You could add some comment for column 4  my_col4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_desc = pd.DataFrame(\n",
    "    {'name': [\"my_col{}\".format(c) for c in range(0, cols)],\n",
    "     'comment': [\"You could add some comment for column {}\".format(c) for c in range(0, cols)]\n",
    "    })\n",
    "\n",
    "col_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allow efficient access and compression when writing data to disk to HDF5 files, data can be split into \"chunks\".\n",
    "See also https://support.hdfgroup.org/HDF5/doc/Advanced/Chunking/index.html\n",
    "\n",
    "Poorly chosen chunksizes/shapes can have a significantly negative impact on performance, so it's usually worth experimenting a bit. The `ptrepack` tool from the `pytables` package is very useful for this purpose in general. `ratschlab-common` comes with a specialized tool for `bigmatrix` files (see example at the end of the notebook).\n",
    "\n",
    "In general, when writing HDF5 files incrementally, it is important to select a chunksize/shape manually, as otherwise the HDF5 library (e.g. pytables) may choose an inappropriate size depending on the first batch of data written.\n",
    "\n",
    "For some advice on how to choose the chunksize, see http://www.pytables.org/usersguide/optimization.html#understanding-chunking\n",
    "Since blosc compression is currently used as default one should aim at something in the ballpark of 500kb of data per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:42.233915Z",
     "start_time": "2018-05-24T08:20:42.227562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkshape = (1000, 50) # nicely aligned with the fake data (doesn't need to be the case)\n",
    "\n",
    "chunksize_kb = np.dtype('float').itemsize * chunkshape[0] * chunkshape[1]/1024\n",
    "chunksize_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T10:59:30.584086Z",
     "start_time": "2018-05-15T10:59:30.577816Z"
    }
   },
   "source": [
    "Since we assume we are dealing with a lot of data, we are going to write the patient data sequentially.\n",
    "\n",
    "Note, that only rows can be appended, but not columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:43.990823Z",
     "start_time": "2018-05-24T08:20:42.279506Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = TablesBigMatrixWriter()\n",
    "\n",
    "if output_path.exists():\n",
    "    output_path.unlink()\n",
    "    \n",
    "for pid in patient_ids:\n",
    "    data, row_desc = fake_patient_data(pid, rows_per_patient, cols)\n",
    "    \n",
    "    writer.write_or_append(output_path, data, row_desc, col_desc, chunkshape=chunkshape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:44.122955Z",
     "start_time": "2018-05-24T08:20:43.993266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 marc  staff  38409169 May 24 10:20 /var/folders/ry/d9fpyr5d21xgbjyrl084ssqh0000gn/T/tmptkcxmkl4/fake_patient_data.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l $output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T08:17:32.224120Z",
     "start_time": "2018-05-16T08:17:32.216119Z"
    }
   },
   "source": [
    "Below the groups in the newly created HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:44.941646Z",
     "start_time": "2018-05-24T08:20:44.126263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\r\n",
      "/data (EArray(50000, 150), fletcher32, shuffle, blosc:lz4(5)) ''\r\n",
      "/col_descr (Group) ''\r\n",
      "/col_descr/table (Table(150,), shuffle, blosc:lz4(5)) ''\r\n",
      "/row_descr (Group) ''\r\n",
      "/row_descr/table (Table(50000,), shuffle, blosc:lz4(5)) ''\r\n"
     ]
    }
   ],
   "source": [
    "!ptdump $output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T08:16:30.951913Z",
     "start_time": "2018-04-06T08:16:30.915373Z"
    }
   },
   "source": [
    "## Reading Back Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:44.954166Z",
     "start_time": "2018-05-24T08:20:44.945791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26710"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading back data of a \"random\" patient\n",
    "some_id = patient_ids[-1]\n",
    "some_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:45.022182Z",
     "start_time": "2018-05-24T08:20:44.957886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26710.20143677, 26710.55182748, 26710.87855975, ...,\n",
       "        26710.81713081, 26710.73815005, 26710.11845951],\n",
       "       [26710.78651242, 26710.69449985, 26710.29312988, ...,\n",
       "        26710.6971596 , 26710.43853235, 26710.15546655],\n",
       "       [26710.84996235, 26710.45633331, 26710.99427889, ...,\n",
       "        26710.87577065, 26710.38067227, 26710.93668617],\n",
       "       ...,\n",
       "       [26710.3633043 , 26710.80679326, 26710.86311316, ...,\n",
       "        26710.04492171, 26710.37460093, 26710.53402499],\n",
       "       [26710.65226909, 26710.07424714, 26710.88747023, ...,\n",
       "        26710.22762388, 26710.1339891 , 26710.48514644],\n",
       "       [26710.02089059, 26710.13063079, 26710.80944825, ...,\n",
       "        26710.5185127 , 26710.93133834, 26710.50079785]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting all the data of some patient\n",
    "with TablesBigMatrixReader(output_path) as reader:\n",
    "    # first getting the row indices of the data belonging to the given patient\n",
    "    row_indices = reader.get_row_indices('id == {}'.format(some_id))\n",
    "    \n",
    "    # accessing the matrix\n",
    "    data_tmp = reader[row_indices, :]\n",
    "data_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:45:19.739030Z",
     "start_time": "2018-04-11T14:45:19.701440Z"
    }
   },
   "source": [
    "We can also get data on a specific day. The code uses pandas' `dataframe.query` function. Query syntax is a bit cumbersome...\n",
    "\n",
    "Note that '2018-03-02' implicitly gets converted to '2018-03-02 00:00:00' local time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:45.086259Z",
     "start_time": "2018-05-24T08:20:45.024798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49289   2018-03-02 00:05:00\n",
       " 49290   2018-03-02 00:10:00\n",
       " 49291   2018-03-02 00:15:00\n",
       " 49292   2018-03-02 00:20:00\n",
       " 49293   2018-03-02 00:25:00\n",
       " 49294   2018-03-02 00:30:00\n",
       " 49295   2018-03-02 00:35:00\n",
       " 49296   2018-03-02 00:40:00\n",
       " 49297   2018-03-02 00:45:00\n",
       " Name: ts, dtype: datetime64[ns],\n",
       " array([[26710.90660873, 26710.8418034 , 26710.71130172, ...,\n",
       "         26710.28144534, 26710.50091314, 26710.55610162],\n",
       "        [26710.72233082, 26710.00978424, 26710.93947081, ...,\n",
       "         26710.28969285, 26710.95075967, 26710.31592936],\n",
       "        [26710.22408737, 26710.19909297, 26710.86823822, ...,\n",
       "         26710.44479405, 26710.02135051, 26710.70572147],\n",
       "        ...,\n",
       "        [26710.312206  , 26710.72083411, 26710.4207162 , ...,\n",
       "         26710.16075147, 26710.0129967 , 26710.90915809],\n",
       "        [26710.40858287, 26710.4680696 , 26710.54513204, ...,\n",
       "         26710.82508201, 26710.99749598, 26710.82239922],\n",
       "        [26710.41591162, 26710.45859962, 26710.34461571, ...,\n",
       "         26710.86537393, 26710.53132655, 26710.16765669]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with TablesBigMatrixReader(output_path) as reader:\n",
    "    row_indices = reader.get_row_indices(\"id == {} & ts >= '2018-03-02' & ts < '2018-03-03'\".format(some_id))\n",
    "    \n",
    "    row_desc = reader.row_desc()\n",
    "    timestamps = row_desc.loc[row_indices, 'ts']\n",
    "    \n",
    "    data_tmp = reader[row_indices, :]\n",
    "timestamps[1:10], data_tmp[1:10, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:45.136057Z",
     "start_time": "2018-05-24T08:20:45.089153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26710.29889554, 26710.66328001],\n",
       "       [26710.38902207, 26710.82468611],\n",
       "       [26710.91017141, 26710.85773244],\n",
       "       ...,\n",
       "       [26710.9989812 , 26710.22916032],\n",
       "       [26710.85948349, 26710.57984862],\n",
       "       [26710.30660865, 26710.45473952]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting only a few columns of data for some patient\n",
    "with TablesBigMatrixReader(output_path) as reader:\n",
    "    row_indices = reader.get_row_indices('id == {}'.format(some_id))\n",
    "    col_indices = reader.get_col_indices_by_name(['my_col5', 'my_col6'])\n",
    "    \n",
    "    data_tmp = reader[row_indices, col_indices]\n",
    "data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:45.143983Z",
     "start_time": "2018-05-24T08:20:45.139140Z"
    }
   },
   "outputs": [],
   "source": [
    "def median_of_column(path, col_name):\n",
    "    # calculating the median over an entire column (i.e. over all patients)\n",
    "    with TablesBigMatrixReader(path) as reader:\n",
    "        col_indices = reader.get_col_indices_by_name([col_name])\n",
    "\n",
    "        return np.median(reader[:, col_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:45.228178Z",
     "start_time": "2018-05-24T08:20:45.147521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.3 ms, sys: 7.15 ms, total: 72.4 ms\n",
      "Wall time: 72.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40353.499702672576"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time median_of_column(output_path, 'my_col7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the chunkshape\n",
    "\n",
    "`ratschlab_common` includes a simple command line tool to tune the chunkshape of an existing h5 file written by `TablesBigMatrixWriter`. It is a wrapper around the `ptrepack` utility (see also http://www.pytables.org/usersguide/utilities.html#ptrepack). It will rewrite data to a new file using the specified chunkshape for the matrix and by default just copy over the metadata to the new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:46.880749Z",
     "start_time": "2018-05-24T08:20:45.231830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: bigmatrix_repack [OPTIONS] SOURCE_FILE DEST_FILE\r\n",
      "\r\n",
      "  Repacks bigmatrix h5 files using pytables' ptrepack.\r\n",
      "\r\n",
      "  Main use case is to experiment with different chunkshapes for the matrix.\r\n",
      "\r\n",
      "Options:\r\n",
      "  --matrix-chunkshape TEXT        Chunkshape of the matrix. Can be a tuple\r\n",
      "                                  like \"(10, 20)\" or \"keep\" or \"auto\"\r\n",
      "  --matrix-repack-options TEXT    options passed to ptrepack for the matrix\r\n",
      "                                  node. If set, the matrix-chunkshape option\r\n",
      "                                  is ignored.\r\n",
      "  --row-meta-repack-options TEXT  options passed to ptrepack for the row\r\n",
      "                                  metadata node.\r\n",
      "  --col-meta-repack-options TEXT  options passed to ptrepack for the column\r\n",
      "                                  metadata node.\r\n",
      "  --force                         If set, destination file will be overwritten\r\n",
      "                                  if it exists\r\n",
      "  --help                          Show this message and exit.\r\n"
     ]
    }
   ],
   "source": [
    "!bigmatrix_repack --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the chunk shape for columnar access pattern using\n",
    "slabs that thin is a bit extreme. But we can roughly read 3-4 times faster. Note, that this may not always work that well also writing times are larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:52.278813Z",
     "start_time": "2018-05-24T08:20:46.884638Z"
    }
   },
   "outputs": [],
   "source": [
    "origin = str(output_path)\n",
    "dest = Path(temp_dir.name, 'repacked.h5')\n",
    "\n",
    "!bigmatrix_repack --matrix-chunkshape \"(5000,1)\" --force $output_path $dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:53.159250Z",
     "start_time": "2018-05-24T08:20:52.283063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\r\n",
      "/data (EArray(50000, 150), fletcher32, shuffle, blosc:lz4(5)) ''\r\n",
      "  atom := Float64Atom(shape=(), dflt=0.0)\r\n",
      "  maindim := 0\r\n",
      "  flavor := 'numpy'\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := (5000, 1)\r\n",
      "/col_descr (Group) ''\r\n",
      "/col_descr/table (Table(150,), shuffle, blosc:lz4(5)) ''\r\n",
      "  description := {\r\n",
      "  \"index\": Int64Col(shape=(), dflt=0, pos=0),\r\n",
      "  \"values_block_0\": StringCol(itemsize=41, shape=(2,), dflt=b'', pos=1)}\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := (728,)\r\n",
      "/row_descr (Group) ''\r\n",
      "/row_descr/table (Table(50000,), shuffle, blosc:lz4(5)) ''\r\n",
      "  description := {\r\n",
      "  \"index\": Int64Col(shape=(), dflt=0, pos=0),\r\n",
      "  \"values_block_0\": Int64Col(shape=(1,), dflt=0, pos=1),\r\n",
      "  \"values_block_1\": Int64Col(shape=(1,), dflt=0, pos=2)}\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := (2730,)\r\n"
     ]
    }
   ],
   "source": [
    "!ptdump -v $dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:53.194879Z",
     "start_time": "2018-05-24T08:20:53.163333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.2 ms, sys: 6.12 ms, total: 25.3 ms\n",
      "Wall time: 23.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40353.499702672576"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time median_of_column(dest, 'my_col7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T08:20:53.209921Z",
     "start_time": "2018-05-24T08:20:53.197734Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_dir.cleanup() # clean up of temporary data"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
